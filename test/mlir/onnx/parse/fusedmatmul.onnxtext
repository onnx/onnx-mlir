// RUN: onnx-mlir --useOnnxModelTypes=false --EmitONNXIR --printIR %s | FileCheck %s

// fusedmatmul.onnxtext is an onnx model with a single FusedMatMul node.
//
// onnx-mlir --useOnnxModelTypes=false --EmitONNXBasic --printIR fusedmatmul.onnxtext outputs a custom op:
// Since onnx can not infer the shape for custom op, option --useOnnxModelTypes=false is added
//   %0 = "onnx.Custom"(%arg0, %arg1) {alpha = 1.250000e-01 : f32, domain_name = "com.microsoft", function_name = "FusedMatMul", transA = 0 : si64, transB = 1 : si64} : (tensor<2x3xf32>, tensor<4x3xf32>) -> tensor<2x4xf32>
//
// onnx-mlir --EmitONNXIR --printIR fusedmatmul.onnxtext decomposes the custom op:
//   %0 = onnx.Constant dense<1.250000e-01> : tensor<1xf32>
//   %1 = "onnx.Transpose"(%arg1) {perm = [1, 0]} : (tensor<4x3xf32>) -> tensor<3x4xf32>
//   %2 = "onnx.MatMul"(%arg0, %1) : (tensor<2x3xf32>, tensor<3x4xf32>) -> tensor<2x4xf32>
//   %3 = "onnx.Mul"(%2, %0) : (tensor<2x4xf32>, tensor<1xf32>) -> tensor<2x4xf32>

<
   ir_version: 8,
   opset_import: ["" : 18]
>
fusedmatmuller (float[2,3] lhs, float[4,3] rhs) => (float[2,4] output) {
   output = com.microsoft.FusedMatMul <alpha = 0.125, transA = 0, transB = 1> (lhs, rhs)
}
// CHECK-LABEL: func.func @main_graph
// CHECK-SAME:  ([[PARAM_0_:%.+]]: tensor<2x3xf32>, [[PARAM_1_:%.+]]: tensor<4x3xf32>) -> tensor<2x4xf32> {{.*}} {
// CHECK:         [[VAR_0_:%.+]] = onnx.Constant dense<1.250000e-01> : tensor<1xf32>
// CHECK:         [[VAR_1_:%.+]] = "onnx.Transpose"([[PARAM_1_]]) {perm = [1, 0]} : (tensor<4x3xf32>) -> tensor<3x4xf32>
// CHECK:         [[VAR_2_:%.+]] = "onnx.MatMul"([[PARAM_0_]], [[VAR_1_]]) : (tensor<2x3xf32>, tensor<3x4xf32>) -> tensor<2x4xf32>
// CHECK:         [[VAR_3_:%.+]] = "onnx.Mul"([[VAR_2_]], [[VAR_0_]]) : (tensor<2x4xf32>, tensor<1xf32>) -> tensor<2x4xf32>
// CHECK:         return [[VAR_3_]] : tensor<2x4xf32>
// CHECK:       }
